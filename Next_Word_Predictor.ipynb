{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdNfcpoNPk5n"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcY0ZfSUQphZ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install demoji\n",
        "\n",
        "import re\n",
        "import demoji\n",
        "import random\n",
        "import inflect\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import torch\n",
        "import torchtext\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.functional import one_hot\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9TRr4wd7J6r"
      },
      "outputs": [],
      "source": [
        "# Reading the text\n",
        "file_path = \"/content/drive/MyDrive/human_chat.txt\"\n",
        "with open(file_path, \"r\") as file:\n",
        "  lines = file.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QN3pXbMqJrX"
      },
      "outputs": [],
      "source": [
        "lines"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(lines)"
      ],
      "metadata": {
        "id": "_2gKkj6aO51k",
        "outputId": "1d8d5ba2-e766-479f-807c-0095ef7a79d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1495"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kVyup4TqnDs"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "        # Remove HTML tags\n",
        "        soup = BeautifulSoup(text, 'html.parser')\n",
        "        text = soup.get_text()\n",
        "\n",
        "        p = inflect.engine() #101\n",
        "        demoji.download_codes()\n",
        "        # Remove emojis\n",
        "        text = demoji.replace(text, \"\")\n",
        "\n",
        "        # Remove mentions of \"Human 1\" and \"Human 2\"\n",
        "        text = re.sub(r'\\b(?:Human 1|Human 2)\\b:?', \" \", text)\n",
        "\n",
        "        # Replace numbers with words\n",
        "        text = re.sub(r'\\b\\d+\\b', lambda x: p.number_to_words(x.group()), text)\n",
        "\n",
        "        # Remove special characters, keeping only alphabetic and spaces\n",
        "        text = re.sub('[^a-zA-Z\\s]', ' ', text)\n",
        "\n",
        "        # Replace specific unicode spaces with standard spaces and trim\n",
        "        text = text.replace(u'\\xa0', u' ').replace('\\u200a', ' ').strip()\n",
        "\n",
        "        return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MY9kwOsqqxUd"
      },
      "outputs": [],
      "source": [
        "preprocessed_lines = [preprocess_text(line) for line in lines]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYf5MGSW-JUh"
      },
      "outputs": [],
      "source": [
        "preprocessed_lines[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMN3LUyjrc0h"
      },
      "outputs": [],
      "source": [
        "tokenizer = get_tokenizer('basic_english')\n",
        "tokenized_conv = [tokenizer(conv) for conv in preprocessed_lines]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "482nwRxkSduP"
      },
      "outputs": [],
      "source": [
        "tokenized_conv[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lyc5JZC0ShWe"
      },
      "outputs": [],
      "source": [
        "features_vocab = torchtext.vocab.build_vocab_from_iterator(\n",
        "    tokenized_conv,\n",
        "    min_freq=1,\n",
        "    specials=['<pad>', '<oov>'],\n",
        "    special_first=True\n",
        ")\n",
        "target_vocab = torchtext.vocab.build_vocab_from_iterator(\n",
        "    tokenized_conv,\n",
        "    min_freq=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQbbCysvSi9t"
      },
      "outputs": [],
      "source": [
        "features_vocab_total_words = len(features_vocab)\n",
        "target_vocab_total_words = len(target_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKgolD4F3N4v"
      },
      "outputs": [],
      "source": [
        "print(\"Features Vocab Length\", features_vocab_total_words)\n",
        "print(\"Target_vocab_Length\", target_vocab_total_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBAQ2OHWSrwl"
      },
      "outputs": [],
      "source": [
        "# making ngrams from the conversations\n",
        "def make_ngrams(tokenized_text):\n",
        "    list_ngrams = []\n",
        "    for i in range(1, len(tokenized_text)):\n",
        "        ngram_sequence = tokenized_text[:i+1]\n",
        "        list_ngrams.append(ngram_sequence)\n",
        "    return list_ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgWh4ViPSy32"
      },
      "outputs": [],
      "source": [
        "ngrams_list = []\n",
        "for tokenized_con in tokenized_conv:\n",
        "    ngrams_list.extend(make_ngrams(tokenized_con))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJjUfF2dSqEd"
      },
      "outputs": [],
      "source": [
        "# Add Random oov tokens to let the model handle oov tokens\n",
        "def add_random_oov_tokens(ngram):\n",
        "    for idx, word in enumerate(ngram[:-1]):\n",
        "        if random.uniform(0, 1) < 0.1:\n",
        "            ngram[idx] = '<oov>'\n",
        "    return ngram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVr9hBXJS2Wu"
      },
      "outputs": [],
      "source": [
        "ngrams_list_oov = []\n",
        "for ngram in ngrams_list:\n",
        "    ngrams_list_oov.append(add_random_oov_tokens(ngram))\n",
        "print(any('<oov>' in ngram for ngram in ngrams_list_oov))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUNJF5LISoSt"
      },
      "outputs": [],
      "source": [
        "def text_to_numerical_sequence(tokenized_text):\n",
        "    tokens_list = []\n",
        "    if tokenized_text[-1] in target_vocab.get_itos():\n",
        "        for token in tokenized_text[:-1]:\n",
        "            num_token = features_vocab[token] if token in features_vocab.get_itos() else features_vocab['<oov>']\n",
        "            tokens_list.append(num_token)\n",
        "        num_token = target_vocab[tokenized_text[-1]]\n",
        "        tokens_list.append(num_token)\n",
        "        return tokens_list\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8AHb7cGS5s9"
      },
      "outputs": [],
      "source": [
        "input_sequences = [text_to_numerical_sequence(sequence) for sequence in ngrams_list_oov if text_to_numerical_sequence(sequence)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjcuuCSNS91v"
      },
      "outputs": [],
      "source": [
        "X = [sequence[:-1] for sequence in input_sequences]\n",
        "y = [sequence[-1] for sequence in input_sequences]\n",
        "len(X[0]), y[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Hr2_I9sS_b_"
      },
      "outputs": [],
      "source": [
        "print(X[3],y[3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwYoEYU9TBJV"
      },
      "outputs": [],
      "source": [
        "longest_sequence_feature = max(len(sequence) for sequence in X)\n",
        "longest_sequence_feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pA-euV4TC4V"
      },
      "outputs": [],
      "source": [
        "padded_X = [F.pad(torch.tensor(sequence), (longest_sequence_feature - len(sequence),0), value=0) for sequence in X]\n",
        "padded_X[1], X[2], len(padded_X[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkMSq8I-TEbv"
      },
      "outputs": [],
      "source": [
        "padded_X = torch.stack(padded_X)\n",
        "y = torch.tensor(y)\n",
        "type(y), type(padded_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kb4JfVahbIYM"
      },
      "outputs": [],
      "source": [
        "y_one_hot = one_hot(y, num_classes=target_vocab_total_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_96pscjbJ88"
      },
      "outputs": [],
      "source": [
        "data = TensorDataset(padded_X, y_one_hot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qv1vD3lMbLZb"
      },
      "outputs": [],
      "source": [
        "train_size = int(0.8 * len(data))\n",
        "test_size = len(data) - train_size\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdIjOU9qacSU"
      },
      "outputs": [],
      "source": [
        "train_data, test_data = random_split(data, [train_size, test_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mK0ao8rHaRz7"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pYtgzTGbQYR"
      },
      "outputs": [],
      "source": [
        "class My_BiLSTM(nn.Module):\n",
        "    def __init__(self, features_vocab_total_words, target_vocab_total_words, embedding_dim, hidden_dim):\n",
        "        super(My_BiLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(features_vocab_total_words, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, target_vocab_total_words)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(self.embedding.weight.device)\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        lstm_out = self.dropout(lstm_out)\n",
        "        # Since the LSTM is bidirectional, we concatenate the last hidden state of the forward direction\n",
        "        # and the first hidden state of the backward direction before passing it to the fully connected layer\n",
        "        # For batch_first=True, the last timestep of the forward direction is lstm_out[:, -1, :hidden_dim]\n",
        "        # and the first timestep of the backward direction is lstm_out[:, 0, hidden_dim:]\n",
        "        output = self.fc(torch.cat((lstm_out[:, -1, :hidden_dim], lstm_out[:, 0, hidden_dim:]), dim=1))\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jBc4l8KfuLZ"
      },
      "outputs": [],
      "source": [
        "features_vocab_total_words = 2749\n",
        "target_vocab_total_words = 2747\n",
        "embedding_dim = 128\n",
        "hidden_dim = 200\n",
        "epochs = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-k2sUk9fx8f"
      },
      "outputs": [],
      "source": [
        "model = My_BiLSTM(features_vocab_total_words, target_vocab_total_words, embedding_dim=embedding_dim, hidden_dim=hidden_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ef_bJY6af0Ha"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0009)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KHPAkuhgC8g"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBmx4fGMgEVD"
      },
      "outputs": [],
      "source": [
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-8wzxwmgF_Z"
      },
      "outputs": [],
      "source": [
        "def calculate_topk_accuracy(model, data_loader, k=3):\n",
        "    model.eval()\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in data_loader:\n",
        "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            output = model(batch_x)\n",
        "\n",
        "            # Get top-k predictions\n",
        "            _, predicted_indices = output.topk(k, dim=1)\n",
        "\n",
        "            # Check if the correct label is in the top-k predictions\n",
        "            correct_predictions += torch.any(predicted_indices == torch.argmax(batch_y, dim=1, keepdim=True), dim=1).sum().item()\n",
        "            total_predictions += batch_y.size(0)\n",
        "\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQ3suic9gIqD"
      },
      "outputs": [],
      "source": [
        "all_accuracies = []\n",
        "all_losses = []\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y.argmax(dim=1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "        accuracy = calculate_topk_accuracy(model, train_loader)\n",
        "        print(f'Epoch {epoch}/{epochs}, Loss: {loss.item():.4f}, Train K-Accuracy: {accuracy * 100:.2f}%')\n",
        "        all_accuracies.append(accuracy)\n",
        "        all_losses.append(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1G90IqBGgLWO"
      },
      "outputs": [],
      "source": [
        "epoch_list = [i for i in range(1,epochs,5)]\n",
        "\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8, 4))\n",
        "\n",
        "axes[0].plot(epoch_list, all_accuracies, color='#5a7da9', label='Accuracy', linewidth=3)\n",
        "axes[0].set_xlabel('Epochs')\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].set_title('Accuracy Graph')\n",
        "axes[0].grid(True)\n",
        "\n",
        "axes[1].plot(epoch_list, all_losses, color='#adad3b', label='Accuracy', linewidth=3)\n",
        "axes[1].set_xlabel('Epochs')\n",
        "axes[1].set_ylabel('Loss')\n",
        "axes[1].set_title('Loss Graph')\n",
        "axes[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCrc_WVKhTvV"
      },
      "outputs": [],
      "source": [
        "accuracy = calculate_topk_accuracy(model, test_loader)\n",
        "print(f'Test K-Accuracy: {accuracy * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwcLtKSyhZDU"
      },
      "outputs": [],
      "source": [
        "model_path = '/content/drive/MyDrive/CampusX_Lecture/Day_3/my_bilstm_model.pth'\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f'Model saved to {model_path}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GidWWw3D6z7s"
      },
      "outputs": [],
      "source": [
        "model_path = '/content/drive/MyDrive/CampusX_Lecture/Day_3/my_bilstm_model.pth'\n",
        "torch.save(model, model_path)\n",
        "print(f'Model saved to {model_path}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxNTlLbg2EJU"
      },
      "outputs": [],
      "source": [
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, features_vocab_total_words, target_vocab_total_words, embedding_dim, hidden_dim):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(features_vocab_total_words, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, target_vocab_total_words)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(self.embedding.weight.device)\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        lstm_out = self.dropout(lstm_out)\n",
        "        # Since the LSTM is bidirectional, we concatenate the last hidden state of the forward direction\n",
        "        # and the first hidden state of the backward direction before passing it to the fully connected layer\n",
        "        # For batch_first=True, the last timestep of the forward direction is lstm_out[:, -1, :hidden_dim]\n",
        "        # and the first timestep of the backward direction is lstm_out[:, 0, hidden_dim:]\n",
        "        output = self.fc(torch.cat((lstm_out[:, -1, :hidden_dim], lstm_out[:, 0, hidden_dim:]), dim=1))\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_yEXggh2Ljy"
      },
      "outputs": [],
      "source": [
        "features_vocab_total_words = 2749\n",
        "target_vocab_total_words = 2747\n",
        "embedding_dim = 128\n",
        "hidden_dim = 200\n",
        "saved_model = BiLSTM(features_vocab_total_words, target_vocab_total_words, embedding_dim, hidden_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzf-cfNX4DBC"
      },
      "outputs": [],
      "source": [
        "model_path = '/content/drive/MyDrive/CampusX_Lecture/Day_3/my_bilstm_model.pth'\n",
        "saved_model.load_state_dict(torch.load(model_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ov0VJaXqiY42"
      },
      "outputs": [],
      "source": [
        "def text_to_numerical_sequence_test(tokenized_text):\n",
        "    tokens_list = []\n",
        "    for token in tokenized_text:\n",
        "        num_token = features_vocab[token] if token in features_vocab.get_itos() else features_vocab['<oov>']\n",
        "        tokens_list.append(num_token)\n",
        "    return tokens_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bydFg1GuhoHe"
      },
      "outputs": [],
      "source": [
        "def use_model(input_list, top_k=5):\n",
        "    saved_model.eval()\n",
        "    output_list = []\n",
        "    for data in input_list:\n",
        "        sentence = data[0]\n",
        "        num_words = data[1]\n",
        "        for i in range(num_words):\n",
        "            output_of_model = []\n",
        "            tokenized_input_test = tokenizer(sentence)\n",
        "            tokenized_sequence_input_test = text_to_numerical_sequence_test(tokenized_input_test)\n",
        "            padded_tokenized_sequence_input_test = F.pad(torch.tensor(tokenized_sequence_input_test),\n",
        "                                                         (longest_sequence_feature - len(tokenized_sequence_input_test)-1, 0),\n",
        "                                                         value=0)\n",
        "            output_test_walking = model(padded_tokenized_sequence_input_test.unsqueeze(0))\n",
        "            # Get the top k predictions and their indices\n",
        "            top_predictions = torch.topk(output_test_walking, top_k)\n",
        "            # Iterate over the top predictions\n",
        "            for index in top_predictions.indices.squeeze():\n",
        "                # Lookup the token for each index and append it to the output sentence\n",
        "                sentence_with_predictions = sentence + ' ' + target_vocab.lookup_token(index.item())\n",
        "                output_list.append(sentence_with_predictions)\n",
        "        # Update the input sentence with the last predicted word\n",
        "        sentence = output_list[-1]\n",
        "    return output_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atA6Ylm8iP38"
      },
      "outputs": [],
      "source": [
        "input_test = [['Hi', 1], ['Hello', 1]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSM6G22miUJG"
      },
      "outputs": [],
      "source": [
        "outputs_model = use_model(input_test,5)\n",
        "outputs_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXyUmeNoo3vw"
      },
      "outputs": [],
      "source": [
        "class TextProcessor:\n",
        "    def __init__(self, file_path):\n",
        "        self.file_path = file_path\n",
        "        self.p = inflect.engine()\n",
        "        demoji.download_codes()\n",
        "\n",
        "    def read_file(self):\n",
        "        with open(self.file_path, \"r\") as file:\n",
        "            return file.readlines()\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        # Remove HTML tags\n",
        "        soup = BeautifulSoup(text, 'html.parser')\n",
        "        text = soup.get_text()\n",
        "\n",
        "        # Remove emojis\n",
        "        text = demoji.replace(text, \"\")\n",
        "\n",
        "        # Remove mentions of \"Human 1\" and \"Human 2\"\n",
        "        text = re.sub(r'\\b(?:Human 1|Human 2)\\b:?', \" \", text)\n",
        "\n",
        "        # Replace numbers with words\n",
        "        text = re.sub(r'\\b\\d+\\b', lambda x: self.p.number_to_words(x.group()), text)\n",
        "\n",
        "        # Remove special characters, keeping only alphabetic and spaces\n",
        "        text = re.sub('[^a-zA-Z\\s]', ' ', text)\n",
        "\n",
        "        # Replace specific unicode spaces with standard spaces and trim\n",
        "        text = text.replace(u'\\xa0', u' ').replace('\\u200a', ' ').strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "    def preprocess_lines(self):\n",
        "        lines = self.read_file()\n",
        "        preprocessed_lines = [self.preprocess_text(line) for line in lines]\n",
        "        return preprocessed_lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCNGl2NxpB2f"
      },
      "outputs": [],
      "source": [
        "file_path = \"/content/drive/MyDrive/human_chat.txt\"\n",
        "\n",
        "processor = TextProcessor(file_path)\n",
        "preprocessed_lines = processor.preprocess_lines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3MJ3ubecpFa0"
      },
      "outputs": [],
      "source": [
        "class TextTokenizerAndVectorizer:\n",
        "    def __init__(self, conversations, oov_rate=0.1, specials=['<pad>', '<oov>']):\n",
        "        self.tokenizer = get_tokenizer('basic_english')\n",
        "        self.conversations = conversations\n",
        "        self.oov_rate = oov_rate\n",
        "        self.specials = specials\n",
        "        self.tokenized_text = [self.tokenizer(conv) for conv in conversations]\n",
        "        self.features_vocab = self.build_vocab(special_first=True)\n",
        "        self.target_vocab = self.build_vocab(special_first=False)\n",
        "        self.num_classes = len(self.target_vocab)\n",
        "\n",
        "    def build_vocab(self, special_first=True):\n",
        "        specials = self.specials if special_first else []\n",
        "        return build_vocab_from_iterator(\n",
        "            self.tokenized_text,\n",
        "            min_freq=1,\n",
        "            specials=specials,\n",
        "            special_first=special_first\n",
        "        )\n",
        "\n",
        "    def make_ngrams(self, text):\n",
        "        ngrams = []\n",
        "        for i in range(1, len(text)):\n",
        "            ngram_sequence = text[:i+1]\n",
        "            ngrams.append(ngram_sequence)\n",
        "        return ngrams\n",
        "\n",
        "    def add_random_oov_tokens(self, ngram):\n",
        "        for idx, word in enumerate(ngram[:-1]):\n",
        "            if random.uniform(0, 1) < self.oov_rate:\n",
        "                ngram[idx] = '<oov>'\n",
        "        return ngram\n",
        "\n",
        "    def text_to_numerical_sequence(self, tokenized_text):\n",
        "        tokens_list = []\n",
        "        if tokenized_text[-1] in self.target_vocab.get_itos():\n",
        "            for token in tokenized_text[:-1]:\n",
        "                num_token = self.features_vocab[token] if token in self.features_vocab.get_itos() else self.features_vocab['<oov>']\n",
        "                tokens_list.append(num_token)\n",
        "            num_token = self.target_vocab[tokenized_text[-1]]\n",
        "            tokens_list.append(num_token)\n",
        "            return tokens_list\n",
        "        return None\n",
        "\n",
        "    def process_text(self):\n",
        "        ngrams_list = [ngram for text in self.tokenized_text for ngram in self.make_ngrams(text)]\n",
        "        ngrams_list_oov = [self.add_random_oov_tokens(ngram) for ngram in ngrams_list]\n",
        "        input_sequences = [self.text_to_numerical_sequence(seq) for seq in ngrams_list_oov if self.text_to_numerical_sequence(seq)]\n",
        "\n",
        "        X = [seq[:-1] for seq in input_sequences]\n",
        "        y = [seq[-1] for seq in input_sequences]\n",
        "        longest_sequence_feature = max(len(seq) for seq in X)\n",
        "\n",
        "        padded_X = torch.stack([F.pad(torch.tensor(seq), (longest_sequence_feature - len(seq),0), value=self.features_vocab['<pad>']) for seq in X])\n",
        "        y = torch.tensor(y)\n",
        "        y_one_hot = one_hot(y, num_classes=self.num_classes)\n",
        "\n",
        "        return padded_X, y_one_hot, len(self.features_vocab), len(self.target_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TJzZoaFpJ9U"
      },
      "outputs": [],
      "source": [
        "txt_processor = TextTokenizerAndVectorizer(preprocessed_lines)\n",
        "X, y, features_vocab_total_words, target_vocab_total_words = txt_processor.process_text()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrffCq0xpOn_"
      },
      "outputs": [],
      "source": [
        "# Assuming X and y are your features and targets tensors respectively\n",
        "data = TensorDataset(X, y)\n",
        "\n",
        "# Calculate split sizes\n",
        "total_size = len(data)\n",
        "train_size = int(0.70 * total_size)\n",
        "val_size = int(0.15 * total_size)\n",
        "test_size = total_size - train_size - val_size  # Ensure the sum of splits equals total_size\n",
        "\n",
        "# Split the dataset\n",
        "train_data, val_data, test_data = random_split(data, [train_size, val_size, test_size])\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 32\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWf5rOnspS20"
      },
      "outputs": [],
      "source": [
        "class My_BiLSTM(nn.Module):\n",
        "    def __init__(self, features_vocab_total_words, target_vocab_total_words, embedding_dim, hidden_dim):\n",
        "        super(My_BiLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(features_vocab_total_words, embedding_dim)\n",
        "        # Specify bidirectional=True to use a BiLSTM\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        # Since it's bidirectional, concatenate the hidden states from both directions\n",
        "        self.fc = nn.Linear(hidden_dim * 2, target_vocab_total_words)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(self.embedding.weight.device)\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        lstm_out = self.dropout(lstm_out)\n",
        "        # Since the LSTM is bidirectional, we concatenate the last hidden state of the forward direction\n",
        "        # and the first hidden state of the backward direction before passing it to the fully connected layer\n",
        "        # For batch_first=True, the last timestep of the forward direction is lstm_out[:, -1, :hidden_dim]\n",
        "        # and the first timestep of the backward direction is lstm_out[:, 0, hidden_dim:]\n",
        "        output = self.fc(torch.cat((lstm_out[:, -1, :hidden_dim], lstm_out[:, 0, hidden_dim:]), dim=1))\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxKijxrYpail"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 128\n",
        "hidden_dim = 256\n",
        "epochs = 50\n",
        "model = My_BiLSTM(features_vocab_total_words, target_vocab_total_words, embedding_dim=embedding_dim, hidden_dim=hidden_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6UU7BiwpgxU"
      },
      "outputs": [],
      "source": [
        "class BiLSTMTrainer:\n",
        "    def __init__(self, model, train_loader, val_loader=None, epochs=50, lr=0.0009, device=None, k=3, patience=5):\n",
        "        self.model = model\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.epochs = epochs\n",
        "        self.device = device if device else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(self.device)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "        self.best_val_loss = float('inf')\n",
        "        self.k = k\n",
        "        self.patience = patience\n",
        "\n",
        "    def calculate_topk_accuracy(self, data_loader):\n",
        "        self.model.eval()\n",
        "        correct_predictions = 0\n",
        "        total_predictions = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y in data_loader:\n",
        "                batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)\n",
        "                output = self.model(batch_x)\n",
        "                _, predicted_indices = output.topk(self.k, dim=1)\n",
        "                correct_predictions += torch.any(predicted_indices == torch.argmax(batch_y, dim=1, keepdim=True), dim=1).sum().item()\n",
        "                total_predictions += batch_y.size(0)\n",
        "\n",
        "        accuracy = correct_predictions / total_predictions\n",
        "        return accuracy\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        train_accuracies = []\n",
        "        val_accuracies = []\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        count = 0\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            self.model.train()\n",
        "            epoch_losses = []\n",
        "            for batch_X, batch_y in self.train_loader:\n",
        "                batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)\n",
        "                self.optimizer.zero_grad()\n",
        "                outputs = self.model(batch_X)\n",
        "                loss = self.criterion(outputs, batch_y.argmax(dim=1))\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                epoch_losses.append(loss.item())\n",
        "\n",
        "            avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
        "            train_losses.append(avg_loss)\n",
        "\n",
        "            if self.val_loader:\n",
        "                val_loss = self.validate()\n",
        "                val_losses.append(val_loss)\n",
        "                if val_loss < self.best_val_loss:\n",
        "                    self.best_val_loss = val_loss\n",
        "                    torch.save(model.state_dict(), 'best_model.pth')\n",
        "                    print(f'new best validation loss: {val_loss:.4f}')\n",
        "                    count = 0\n",
        "                else:\n",
        "                    count += 1\n",
        "                    if count > self.patience:\n",
        "                      print(f'Early stopping at epoch {epoch + 1}')\n",
        "                      break  # Early stopping condition\n",
        "\n",
        "            train_accuracy = self.calculate_topk_accuracy(self.train_loader)\n",
        "            val_accuracy = self.calculate_topk_accuracy(self.val_loader)\n",
        "\n",
        "            train_accuracies.append(train_accuracy)\n",
        "            val_accuracies.append(val_accuracy)\n",
        "\n",
        "            if epoch % 5 == 0:\n",
        "                print(f'Epoch {epoch}/{self.epochs} ||Train Loss: {avg_loss:.4f} || Train K-Accuracy: {train_accuracy * 100:.2f}% || Val Loss :{val_loss:.4f} || Validation K-Accuracy: {val_accuracy * 100:.2f}%')\n",
        "\n",
        "        self.plot_results(train_accuracies, train_losses, val_accuracies, val_losses)\n",
        "\n",
        "    def validate(self):\n",
        "        self.model.eval()\n",
        "        val_losses = []\n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y in self.val_loader:\n",
        "                batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)\n",
        "                outputs = self.model(batch_x)\n",
        "                loss = self.criterion(outputs, batch_y.argmax(dim=1))\n",
        "                val_losses.append(loss.item())\n",
        "        return sum(val_losses) / len(val_losses)\n",
        "\n",
        "    def plot_results(self, train_accuracies, train_losses, val_accuracies, val_losses):\n",
        "\n",
        "        epochs_x = list(range(1, len(train_accuracies) + 1))\n",
        "        epochs_x_ = list(range(1, len(train_losses) + 1))\n",
        "        plt.figure(figsize=(12, 6))\n",
        "\n",
        "        # Creating two y-axes\n",
        "        fig, ax1 = plt.subplots()\n",
        "\n",
        "        # Plotting training and validation accuracies\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(epochs_x, train_accuracies, 'b-o', label='Train Accuracy')\n",
        "        plt.plot(epochs_x, val_accuracies, 'r-+', label='Validation Accuracy')\n",
        "        plt.title('Training and Validation Accuracy')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend(loc='best')\n",
        "\n",
        "        # Plotting training and validation losses\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(epochs_x_, train_losses, 'g-o', label='Train Loss')\n",
        "        plt.plot(epochs_x_, val_losses, 'k-+', label='Validation Loss')\n",
        "        plt.title('Training and Validation Loss')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend(loc='best')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwbcPyPypnKw"
      },
      "outputs": [],
      "source": [
        "trainer = BiLSTMTrainer(model, train_loader=train_loader, val_loader=val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vp_WucI_pqlH"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}